{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f27d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/isabe/AppData/Local/Programs/Python/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Quantum Augmentation for Anemia Detection — Clean Pipeline\n",
    "\n",
    "## Part 0: Imports & Model Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb34a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Required imports ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "import os\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError:\n",
    "    lgb = None\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    xgb = None\n",
    "try:\n",
    "    import catboost as cb\n",
    "except ImportError:\n",
    "    cb = None\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as pnp\n",
    "except Exception:\n",
    "    qml = None; pnp = None\n",
    "# Add any other imports needed for your workflow here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01323f",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Global experiment parameters ----\n",
    "# Set these at the top to control all trials\n",
    "N_TRIALS = 3           # Number of cross-validation trials (e.g., seeds)\n",
    "N_FOLDS = 5            # Number of CV folds\n",
    "QRF_D = 8              # QRF feature dimension\n",
    "QRF_REPS = 2           # QRF repetitions\n",
    "QRF_SEED = 11          # QRF random seed\n",
    "QK_D = 6               # Quantum kernel PCA dim\n",
    "QK_M = 12              # Quantum kernel landmarks\n",
    "QK_SEED = 7            # Quantum kernel seed\n",
    "RIDGE_ALPHA = 1.0      # Ridge regression alpha\n",
    "USE_CAT = True         # Use CatBoost\n",
    "USE_XGB = True         # Use XGBoost\n",
    "USE_KDE = True         # Use fold-safe KDE weights\n",
    "USE_EARLY_STOP = False # Use early stopping in blend_cv\n",
    "WINSOR_LOW = 1.0       # WinsorClipper low percentile\n",
    "WINSOR_HIGH = 99.0     # WinsorClipper high percentile\n",
    "\n",
    "# You can add more parameters here as needed for your experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fa94c",
   "metadata": {},
   "source": [
    "## Part 2: Global Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797da242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data loading and preprocessing ----\n",
    "# Mount Google Drive (Colab only; adjust for local paths)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    CHK = '/content/gdrive/MyDrive/hb_checkpoints'\n",
    "    RUN_DIR = '/content/gdrive/MyDrive/anemia_runs'\n",
    "except ImportError:\n",
    "    # Local fallback: adjust paths to your data location\n",
    "    CHK = './hb_checkpoints'\n",
    "    RUN_DIR = './anemia_runs'\n",
    "\n",
    "# Load core arrays (X, y_reg, groups)\n",
    "X      = np.load(os.path.join(CHK, 'X.npy')).astype(np.float32)\n",
    "y_reg  = np.load(os.path.join(CHK, 'y_reg.npy')).astype(np.float32)\n",
    "groups = np.load(os.path.join(CHK, 'groups.npy'), allow_pickle=True).astype(str)\n",
    "\n",
    "print('✓ Data loaded:')\n",
    "print(f'  X shape: {X.shape}  y_reg shape: {y_reg.shape}  groups shape: {groups.shape}')\n",
    "print(f'  First few group IDs: {groups[:5]}')\n",
    "\n",
    "# Load cross-validation splits (must exist)\n",
    "import joblib\n",
    "split_file = [f for f in os.listdir(RUN_DIR) if f.endswith('_splits.pkl')][-1] if os.path.exists(RUN_DIR) else None\n",
    "if split_file:\n",
    "    FIXED_SPLITS = joblib.load(os.path.join(RUN_DIR, split_file))\n",
    "    print(f'✓ Loaded {len(FIXED_SPLITS)} CV folds')\n",
    "else:\n",
    "    # Fallback: create simple GroupKFold splits\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    FIXED_SPLITS = list(gkf.split(X, y_reg, groups))\n",
    "    print(f'✓ Created {len(FIXED_SPLITS)} GroupKFold splits')\n",
    "\n",
    "# Optional: load config and OOF results if available\n",
    "config_file = [f for f in os.listdir(RUN_DIR) if f.endswith('_config.json')][-1] if os.path.exists(RUN_DIR) else None\n",
    "if config_file:\n",
    "    import json\n",
    "    with open(os.path.join(RUN_DIR, config_file)) as f:\n",
    "        cfg = json.load(f)\n",
    "    print(f'✓ Config loaded: {cfg}')\n",
    "else:\n",
    "    print('⚠ No config file found; using defaults')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d825c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WinsorClipper\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class WinsorClipper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Per-feature winsor clipping using percentiles (fold-safe when fit on TRAIN only).\"\"\"\n",
    "    def __init__(self, low_pct=1.0, high_pct=99.0):\n",
    "        self.low_pct = float(low_pct)\n",
    "        self.high_pct= float(high_pct)\n",
    "        self.low_ = None\n",
    "        self.high_= None\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        self.low_ = np.percentile(X, self.low_pct, axis=0)\n",
    "        self.high_ = np.percentile(X, self.high_pct, axis=0)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=float).copy()\n",
    "        if self.low_ is None or self.high_ is None:\n",
    "            raise RuntimeError(\"WinsorClipper not fitted\")\n",
    "        X = np.minimum(np.maximum(X, self.low_), self.high_)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c82b7",
   "metadata": {},
   "source": [
    "## Part 3: Core Utilities — Preprocessing (WinsorClipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccad065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Quantum augmentation: QRFPre + QKernelLandmarksPre + factory ----\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import lru_cache\n",
    "\n",
    "# primitive helpers (angle scaling / random matrix)\n",
    "def angle_scale(z):\n",
    "    z0 = (z - z.mean(axis=0)) / (z.std(axis=0) + 1e-8)\n",
    "    z0 = np.clip(z0, -3.0, 3.0) / 3.0\n",
    "    return z0 * np.pi\n",
    "\n",
    "@lru_cache(maxsize=2048)\n",
    "def _rand_matrix(key, rows, cols):\n",
    "    rng = np.random.default_rng(int(key))\n",
    "    W = rng.normal(0, 1.0, size=(rows, cols)).astype(np.float32)\n",
    "    b = rng.uniform(0, 2*np.pi, size=(rows,)).astype(np.float32)\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def qrand_features_angles(Z_ang, out_dim, seed):\n",
    "    N, D = Z_ang.shape\n",
    "    W, b = _rand_matrix((seed<<16) + D + out_dim, out_dim, D)\n",
    "    return np.cos(Z_ang @ W.T + b[None, :]).astype(np.float32)\n",
    "\n",
    "\n",
    "def kernel_to_landmarks_angles(Z_all, Z_land):\n",
    "    A = Z_all / (np.linalg.norm(Z_all, axis=1, keepdims=True) + 1e-9)\n",
    "    B = Z_land / (np.linalg.norm(Z_land, axis=1, keepdims=True) + 1e-9)\n",
    "    K = (A @ B.T).astype(np.float32)\n",
    "    mu = K.mean(axis=0, keepdims=True)\n",
    "    sd = K.std(axis=0, keepdims=True) + 1e-8\n",
    "    return (K - mu) / sd\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class QRFPre(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, D=8, reps=2, seed=11, concat=True, save_dir=None, tag=\"qrf\", force_random=False):\n",
    "        self.D=D; self.reps=reps; self.seed=seed\n",
    "        self.concat=concat\n",
    "        self.save_dir = save_dir or \"./qcache\"\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.tag=tag\n",
    "        self.force_random = force_random\n",
    "        self.pca_=None\n",
    "        self.theta1_=None; self.theta2_=None\n",
    "        self.fit_uuid_=None\n",
    "        self._printed_once=False\n",
    "    def _angles(self, Z):\n",
    "        Z0 = (Z - Z.mean(axis=0)) / (Z.std(axis=0)+1e-8)\n",
    "        Z0 = np.clip(Z0, -3.0, 3.0)/3.0\n",
    "        return Z0 * np.pi\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X, np.float32)\n",
    "        m = min(self.D, X.shape[1])\n",
    "        self.pca_ = PCA(n_components=m, random_state=0).fit(X)\n",
    "        self.fit_uuid_ = os.urandom(8).hex()\n",
    "        return self\n",
    "    def _qrf(self, Z_ang):\n",
    "        N = Z_ang.shape[0]\n",
    "        out_dim = self.reps * self.D\n",
    "        out = np.zeros((N, out_dim), dtype=np.float32)\n",
    "        for r in range(self.reps):\n",
    "            out[:, r*self.D:(r+1)*self.D] = qrand_features_angles(Z_ang, self.D, self.seed + r)\n",
    "        return out\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, np.float32)\n",
    "        Z = self.pca_.transform(X).astype(np.float32)\n",
    "        Z_ang = self._angles(Z)\n",
    "        arr = self._qrf(Z_ang)\n",
    "        out = np.hstack([X, arr]).astype(np.float32) if self.concat else arr.astype(np.float32)\n",
    "        if not self._printed_once:\n",
    "            print(f\"[QRF] transform shapes: X={X.shape} QRF={arr.shape} OUT={out.shape}\")\n",
    "            self._printed_once=True\n",
    "        return out\n",
    "\n",
    "\n",
    "class QKernelLandmarksPre(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, D=6, M=12, seed=7, concat=True, save_dir=None, tag=\"qk\", use_cosine=True):\n",
    "        self.D=D; self.M=M; self.seed=seed; self.concat=concat\n",
    "        self.save_dir = save_dir or \"./qcache\"; os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.tag=tag; self.use_cosine=use_cosine\n",
    "        self.pca_=None; self.landmark_idx_=None; self.landmarks_=None; self.fit_uuid_=None\n",
    "        self._printed_once=False\n",
    "    def _angles(self, Z):\n",
    "        Z0 = (Z - Z.mean(axis=0)) / (Z.std(axis=0)+1e-8)\n",
    "        Z0 = np.clip(Z0, -3.0, 3.0)/3.0\n",
    "        return Z0 * np.pi\n",
    "    def fit(self, X, y=None):\n",
    "        import uuid\n",
    "        X = np.asarray(X, np.float32)\n",
    "        d = min(self.D, X.shape[1])\n",
    "        from sklearn.decomposition import PCA\n",
    "        self.pca_ = PCA(n_components=d, random_state=0).fit(X)\n",
    "        Z = self.pca_.transform(X).astype(np.float32)\n",
    "        Z_ang = self._angles(Z)\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        if self.M > Z_ang.shape[0]: self.M = Z_ang.shape[0]\n",
    "        self.landmark_idx_ = np.sort(rng.choice(Z_ang.shape[0], size=self.M, replace=False))\n",
    "        self.landmarks_ = Z_ang[self.landmark_idx_].copy()\n",
    "        self.fit_uuid_ = uuid.uuid4().hex\n",
    "        print(f\"[QK] Fit: D={self.D} M={self.M} seed={self.seed} landmarks from TRAIN fold only.\")\n",
    "        return self\n",
    "    def _kernel(self, Xa, Xb):\n",
    "        if self.use_cosine:\n",
    "            Xa = Xa / (np.linalg.norm(Xa, axis=1, keepdims=True) + 1e-8)\n",
    "            Xb = Xb / (np.linalg.norm(Xb, axis=1, keepdims=True) + 1e-8)\n",
    "            return Xa @ Xb.T\n",
    "        diff = Xa[:, None, :] - Xb[None, :, :]\n",
    "        K = np.exp(-0.5 * (diff**2).sum(axis=-1) / max(1e-6, Xa.shape[1]))\n",
    "        return K\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, np.float32)\n",
    "        Z = self.pca_.transform(X).astype(np.float32)\n",
    "        Z_ang = self._angles(Z)\n",
    "        K = self._kernel(Z_ang, self.landmarks_).astype(np.float32)\n",
    "        K = (K - K.mean(axis=0, keepdims=True)) / (K.std(axis=0, keepdims=True) + 1e-6)\n",
    "        out = np.hstack([X, K]).astype(np.float32) if self.concat else K\n",
    "        if not self._printed_once:\n",
    "            print(f\"[QK] transform shapes: X={X.shape} K={K.shape} OUT={out.shape}\")\n",
    "            self._printed_once=True\n",
    "        return out\n",
    "\n",
    "\n",
    "def make_quantum_augmenter(qrf_D=8, qrf_reps=2, qrf_seed=11, qk_D=6, qk_M=12, qk_seed=7, save_dir=None, tag=\"qaug\", force_random=False):\n",
    "    qrf = QRFPre(D=qrf_D, reps=qrf_reps, seed=qrf_seed, concat=False, save_dir=save_dir, tag=f\"{tag}_qrf\", force_random=force_random)\n",
    "    qk  = QKernelLandmarksPre(D=qk_D, M=qk_M, seed=qk_seed, concat=False, save_dir=save_dir, tag=f\"{tag}_qk\")\n",
    "    class QuantumAug(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, qrf, qk):\n",
    "            self.qrf=qrf; self.qk=qk\n",
    "            self.mu_q_=None; self.sig_q_=None; self._printed_once=False\n",
    "        def fit(self, X, y=None):\n",
    "            self.qrf.fit(X, y); self.qk.fit(X, y)\n",
    "            A = self.qrf.transform(np.asarray(X, np.float32))\n",
    "            B = self.qk.transform(np.asarray(X, np.float32))\n",
    "            Q = np.hstack([A, B]).astype(np.float32)\n",
    "            self.mu_q_ = Q.mean(axis=0, keepdims=True)\n",
    "            self.sig_q_ = Q.std(axis=0, keepdims=True) + 1e-6\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            A = self.qrf.transform(X); B = self.qk.transform(X)\n",
    "            Q = np.hstack([A, B]).astype(np.float32)\n",
    "            Q = (Q - self.mu_q_) / self.sig_q_\n",
    "            out = np.hstack([X, Q]).astype(np.float32)\n",
    "            if not self._printed_once:\n",
    "                print(f\"[QAug] Shapes: X={X.shape}  QRF={A.shape}  QK={B.shape}  OUT={out.shape}\")\n",
    "                self._printed_once=True\n",
    "            return out\n",
    "    return QuantumAug(qrf=qrf, qk=qk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3e369",
   "metadata": {},
   "source": [
    "## Part 4: Quantum Augmentation Classes (QRFPre, QKernelLandmarksPre, Factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cff1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Canonical blend_cv (single copy, fold-safe pre) ----\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Note: this is a concise but feature-matching version of your full blend_cv\n",
    "\n",
    "def blend_cv(\n",
    "    X, y, groups, splits,\n",
    "    lgbm_seeds=(0,1,2), use_cat=True, use_xgb=True,\n",
    "    pre=None, feat_df=None, light_feat_cols=(), ridge_alpha=1.0,\n",
    "    use_fold_kde_weights=False, use_early_stopping=False\n",
    "):\n",
    "    X = np.asarray(X, np.float32)\n",
    "    y = np.asarray(y, np.float32)\n",
    "    n = len(y)\n",
    "    y_oof = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    use_residual = (feat_df is not None) and (len(light_feat_cols) > 0)\n",
    "\n",
    "    for fold_id,(tr,va) in enumerate(splits,1):\n",
    "        t0=time.time()\n",
    "        Xtr_raw, Xva_raw = X[tr], X[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        if pre is not None:\n",
    "            pre_local = Pipeline(pre.steps) if isinstance(pre, Pipeline) else pre\n",
    "            pre_local.fit(Xtr_raw, ytr)\n",
    "            Xtr = pre_local.transform(Xtr_raw)\n",
    "            Xva = pre_local.transform(Xva_raw)\n",
    "        else:\n",
    "            Xtr, Xva = Xtr_raw, Xva_raw\n",
    "\n",
    "        preds=[]; weights=[]\n",
    "        # LGBM seeds (use your builder from the original notebook if present)\n",
    "        try:\n",
    "            for sd in lgbm_seeds:\n",
    "                m = lgbm_builder(sd)\n",
    "                m.fit(_to_df_with_cols(Xtr), ytr)\n",
    "                pv = m.predict(_to_df_with_cols(Xva))\n",
    "                preds.append(pv); weights.append(1.0)\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        # XGB\n",
    "        try:\n",
    "            if use_xgb:\n",
    "                mx = xgb_builder(0)\n",
    "                mx.fit(Xtr, ytr)\n",
    "                pv = mx.predict(Xva)\n",
    "                preds.append(pv); weights.append(1.0)\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        # Cat\n",
    "        try:\n",
    "            if use_cat:\n",
    "                mc = cat_builder(0)\n",
    "                mc.fit(Xtr, ytr, verbose=False)\n",
    "                pv = mc.predict(Xva)\n",
    "                preds.append(pv); weights.append(1.0)\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        if len(preds)==0:\n",
    "            raise RuntimeError(\"No base models available in blend_cv: define lgbm_builder/xgb_builder/cat_builder or set flags\")\n",
    "\n",
    "        P = np.vstack(preds); w = np.asarray(weights).reshape(-1,1)\n",
    "        p_blend = (P * w).sum(axis=0) / (w.sum() + 1e-12)\n",
    "\n",
    "        if use_residual:\n",
    "            Ztr = feat_df.iloc[tr][list(light_feat_cols)].to_numpy(dtype=np.float32)\n",
    "            Zva = feat_df.iloc[va][list(light_feat_cols)].to_numpy(dtype=np.float32)\n",
    "            m_tmp = lgbm_builder(999); m_tmp.fit(_to_df_with_cols(Xtr), ytr)\n",
    "            p_tr_tmp = m_tmp.predict(_to_df_with_cols(Xtr))\n",
    "            res_tr = ytr - p_tr_tmp\n",
    "            rb = Ridge(alpha=float(ridge_alpha)).fit(Ztr, res_tr)\n",
    "            p_blend = p_blend + rb.predict(Zva)\n",
    "\n",
    "        y_oof[va] = p_blend\n",
    "\n",
    "    score_oof = normalize_score_from_preds(y_oof)\n",
    "    return {\"y_oof\": y_oof, \"score_oof\": score_oof, \"meta\": {\"n_folds\": len(splits)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper functions for blend_cv ----\n",
    "def normalize_score_from_preds(y_pred):\n",
    "    \"\"\"Convert predictions to normalized [0,1] score: lower values = higher risk.\"\"\"\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    s = -y_pred  # invert (lower Hb = higher risk)\n",
    "    smin, smax = float(s.min()), float(s.max())\n",
    "    denom = (smax - smin) if (smax > smin) else 1.0\n",
    "    return (s - smin) / denom\n",
    "\n",
    "def _to_df_with_cols(X):\n",
    "    \"\"\"Convert numpy array to DataFrame with auto-generated column names (for LGBM).\"\"\"\n",
    "    import pandas as pd\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    cols = [f'f{i}' for i in range(X.shape[1])]\n",
    "    return pd.DataFrame(X, columns=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744c708",
   "metadata": {},
   "source": [
    "## Part 5: Canonical Cross-Validation Pipeline (blend_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662941d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Example usage: Running the pipeline with quantum augmentation ----\n",
    "# Uncomment and adjust to use in your workflow:\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# quant_aug = make_quantum_augmenter(qrf_D=8,qrf_reps=2,qrf_seed=11,qk_D=6,qk_M=12,qk_seed=7, save_dir='./qcache', tag='hybrid')\n",
    "# pre = Pipeline([('winsor', WinsorClipper(WINSOR_LOW, WINSOR_HIGH)), ('qaug', quant_aug)])\n",
    "# bag = blend_cv(X, y_reg, groups, FIXED_SPLITS, pre=pre, lgbm_seeds=(0,1,2), use_cat=USE_CAT, use_xgb=USE_XGB, feat_df=None, light_feat_cols=[])\n",
    "\n",
    "# The output `bag` is a dict with keys: 'y_oof', 'score_oof', 'meta'\n",
    "# You can then evaluate metrics or save the OOF predictions for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71198c9f",
   "metadata": {},
   "source": [
    "## Part 7: Metrics & Threshold Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aab177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as pnp\n",
    "except Exception:\n",
    "    qml = None; pnp = None\n",
    "\n",
    "class TrainableQRFPre(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Experimental trainable QRF wrapper. Requires PennyLane.\n",
    "    Kept here for reproducibility; not required for main paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, D=8, reps=2, seed=11, epochs=30, lr=0.05, concat=True, tag=\"tqrf\"):\n",
    "        self.D=D; self.reps=reps; self.seed=seed\n",
    "        self.epochs=epochs; self.lr=lr; self.concat=concat; self.tag=tag\n",
    "        self.pca_=None; self.fit_uuid_=None; self._printed_once=False\n",
    "        self.params_ = None\n",
    "\n",
    "    def _angles(self, Z):\n",
    "        Z0 = (Z - Z.mean(axis=0)) / (Z.std(axis=0)+1e-8)\n",
    "        Z0 = np.clip(Z0, -3.0, 3.0)/3.0\n",
    "        return Z0 * np.pi\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if qml is None:\n",
    "            raise RuntimeError(\"PennyLane not available for TrainableQRFPre.\")\n",
    "        if y is None:\n",
    "            raise ValueError(\"TrainableQRFPre.fit requires y (regression target) to train.\")\n",
    "        # Minimal: perform PCA then simulate a trainable param vector via random init\n",
    "        from sklearn.decomposition import PCA\n",
    "        X = np.asarray(X, np.float32)\n",
    "        m = min(self.D, X.shape[1])\n",
    "        self.pca_ = PCA(n_components=m, random_state=0).fit(X)\n",
    "        # placeholder: store dummy params (you can replace with PL training loop)\n",
    "        self.params_ = np.random.default_rng(self.seed).standard_normal(self.D * self.reps).astype(np.float32)\n",
    "        self.fit_uuid_ = os.urandom(8).hex()\n",
    "        print(f\"[TrainableQRF] fitted (EXPERIMENTAL) D={self.D} reps={self.reps}\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, np.float32)\n",
    "        Z = self.pca_.transform(X).astype(np.float32)\n",
    "        Z_ang = self._angles(Z)\n",
    "        # deterministic surrogate using qrand_features_angles but shifted by params_\n",
    "        arr = qrand_features_angles(Z_ang, self.D, self.seed)\n",
    "        out = np.hstack([X, arr]).astype(np.float32) if self.concat else arr.astype(np.float32)\n",
    "        if not self._printed_once:\n",
    "            print(f\"[TrainableQRF] transform shapes: X={X.shape} OUT={out.shape} (EXPERIMENTAL)\")\n",
    "            self._printed_once=True\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Metrics, Thresholding, and Reporting Functions ----\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "\n",
    "def _norm_score(y_pred_hb):\n",
    "    \"\"\"Normalize predictions to [0,1] risk score (lower Hb = higher risk).\"\"\"\n",
    "    s = -np.asarray(y_pred_hb, dtype=float)\n",
    "    s_min, s_max = float(s.min()), float(s.max())\n",
    "    denom = (s_max - s_min) if (s_max > s_min) else 1.0\n",
    "    return (s - s_min) / denom\n",
    "\n",
    "def clinical_metrics(y_true_hb, y_pred_hb, hb_cut=12.5):\n",
    "    \"\"\"Compute metrics at clinical threshold Hb < 12.5 g/dL.\"\"\"\n",
    "    y_true = (np.asarray(y_true_hb) < float(hb_cut)).astype(int)\n",
    "    y_hat  = (np.asarray(y_pred_hb) < float(hb_cut)).astype(int)\n",
    "    cm  = confusion_matrix(y_true, y_hat, labels=[1,0])\n",
    "    sen = recall_score(y_true, y_hat, pos_label=1)\n",
    "    spec= recall_score(y_true, y_hat, pos_label=0)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    bal = 0.5*(sen+spec)\n",
    "    score = _norm_score(y_pred_hb)\n",
    "    auroc = roc_auc_score(y_true, score) if len(np.unique(y_true))==2 else np.nan\n",
    "    prauc = average_precision_score(y_true, score) if len(np.unique(y_true))==2 else np.nan\n",
    "    return {\"ACC\":acc,\"BAL_ACC\":bal,\"SEN\":sen,\"SPEC\":spec,\"AUROC\":auroc,\"PRAUC\":prauc}, cm\n",
    "\n",
    "def pick_t(score, y_true_hb, mode=\"balanced\"):\n",
    "    \"\"\"Find optimal probability threshold.\"\"\"\n",
    "    ts = np.linspace(0.2, 0.8, 61)\n",
    "    best_t, best_val = 0.5, -1.0\n",
    "    for t in ts:\n",
    "        m_t, _ = metrics_at_t(y_true_hb, score, t)\n",
    "        val = m_t[\"BAL_ACC\"] if mode==\"balanced\" else m_t[\"ACC\"]\n",
    "        if val > best_val:\n",
    "            best_val, best_t = val, t\n",
    "    return best_t\n",
    "\n",
    "def metrics_at_t(y_true_hb, score, t):\n",
    "    \"\"\"Compute metrics at a specific probability threshold t.\"\"\"\n",
    "    y_true = (np.asarray(y_true_hb) < 12.5).astype(int)\n",
    "    y_hat  = (np.asarray(score) >= float(t)).astype(int)\n",
    "    cm  = confusion_matrix(y_true, y_hat, labels=[1,0])\n",
    "    sen = recall_score(y_true, y_hat, pos_label=1)\n",
    "    spec= recall_score(y_true, y_hat, pos_label=0)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    bal = 0.5*(sen+spec)\n",
    "    auroc = roc_auc_score(y_true, score)\n",
    "    prauc = average_precision_score(y_true, score)\n",
    "    return {\"ACC\":acc,\"BAL_ACC\":bal,\"SEN\":sen,\"SPEC\":spec,\"AUROC\":auroc,\"PRAUC\":prauc}, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 8: Run Quantum Augmentation & Report Results\n",
    "\n",
    "# This cell runs the canonical quantum-augmentation experiments end-to-end.\n",
    "# Paste & Run in Colab — it will execute N_TRIALS trials and print summary tables.\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Provide simple model builders if the notebook does not define them already\n",
    "try:\n",
    "    lgbm_builder\n",
    "except NameError:\n",
    "    def lgbm_builder(seed=0):\n",
    "        if lgb is not None:\n",
    "            return lgb.LGBMRegressor(n_estimators=200, random_state=int(seed))\n",
    "        else:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            return RandomForestRegressor(n_estimators=100, random_state=int(seed))\n",
    "try:\n",
    "    xgb_builder\n",
    "except NameError:\n",
    "    def xgb_builder(seed=0):\n",
    "        if xgb is not None:\n",
    "            return xgb.XGBRegressor(n_estimators=200, random_state=int(seed), verbosity=0)\n",
    "        else:\n",
    "            from sklearn.linear_model import Ridge\n",
    "            return Ridge()\n",
    "try:\n",
    "    cat_builder\n",
    "except NameError:\n",
    "    def cat_builder(seed=0):\n",
    "        if cb is not None:\n",
    "            return cb.CatBoostRegressor(iterations=200, verbose=0, random_seed=int(seed))\n",
    "        else:\n",
    "            from sklearn.linear_model import Ridge\n",
    "            return Ridge()\n",
    "\n",
    "# Run trials: create quant augmenter with different seeds and run blend_cv\n",
    "results = []\n",
    "for trial in range(max(1, int(N_TRIALS))):\n",
    "    t0 = time.time()\n",
    "    trial_seed = int(QRF_SEED) + int(trial)\n",
    "    print(f'\\n=== Trial {trial+1}/{N_TRIALS} (seed={trial_seed}) ===')\n",
    "    quant_aug = make_quantum_augmenter(qrf_D=QRF_D, qrf_reps=QRF_REPS, qrf_seed=trial_seed, qk_D=QK_D, qk_M=QK_M, qk_seed=QK_SEED, save_dir='./qcache', tag=f'QAug_seed{trial_seed}')\n",
    "    pre_qaug = Pipeline([('winsor', WinsorClipper(WINSOR_LOW, WINSOR_HIGH)), ('qaug', quant_aug)])\n",
    "    try:\n",
    "        bag_qaug = blend_cv(X, y_reg, groups, FIXED_SPLITS, pre=pre_qaug, lgbm_seeds=(0,1,2), use_cat=USE_CAT, use_xgb=USE_XGB, feat_df=None, light_feat_cols=[])\n",
    "    except Exception as e:\n",
    "        print('Trial failed:', e)\n",
    "        continue\n",
    "    y_oof_qaug = bag_qaug['y_oof']\n",
    "    score_qaug = bag_qaug['score_oof']\n",
    "    m_clin_qaug, cm_clin_qaug = clinical_metrics(y_reg, y_oof_qaug)\n",
    "    t_bal_qaug = pick_t(score_qaug, y_reg, mode='balanced')\n",
    "    m_bal_qaug, cm_bal_qaug = metrics_at_t(y_reg, score_qaug, t_bal_qaug)\n",
    "    tbl_qaug = pd.DataFrame([{'Operating point': 'Clinical (Hb<12.5)', **m_clin_qaug}, {'Operating point': f'OOF-balanced (t={t_bal_qaug:.3f})', **m_bal_qaug}]).round(3)\n",
    "    print('\\n=== Quantum-Augmented (QRF + Kernel) Results ===')\n",
    "    print(tbl_qaug)\n",
    "    print('\\nConfusion Matrix (Clinical [[TP,FN],[FP,TN]]):\\n', cm_clin_qaug)\n",
    "    print('\\nConfusion Matrix (OOF-balanced [[TP,FN],[FP,TN]]):\\n', cm_bal_qaug)\n",
    "    results.append({'seed': trial_seed, 'tbl': tbl_qaug, 'm_clin': m_clin_qaug, 'm_bal': m_bal_qaug})\n",
    "    print(f'Trial time: {time.time()-t0:.1f}s')\n",
    "\n",
    "# Summary across trials\n",
    "if len(results) > 1:\n",
    "    print('\\n=== Summary across trials ===')\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        row = {'seed': r['seed'], **{f'clin_{k}':v for k,v in r['m_clin'].items()}, **{f'bal_{k}':v for k,v in r['m_bal'].items()}}\n",
    "        rows.append(row)\n",
    "    df_sum = pd.DataFrame(rows).set_index('seed')\n",
    "    print(df_sum.round(3))\n",
    "\n",
    "# Optionally save OOF predictions (best-effort)\n",
    "try:\n",
    "    np.save(os.path.join(RUN_DIR, 'qaug_oof_summary.npy'), np.stack([r['tbl'].to_numpy() for r in results]))\n",
    "    print('Saved OOF summary to RUN_DIR')\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff505874",
   "metadata": {},
   "source": [
    "# EXTRA / OPTIONAL ABLATION CODE\n",
    "\n",
    "The cells below contain **optional experimental code** for ablation studies and alternative approaches:\n",
    "- **TrainableQRFPre**: Experimental trainable quantum random features (requires PennyLane + training loop)\n",
    "- **FidelityKernelLandmarksPre**: Fidelity-based approximation of quantum kernel landmarks\n",
    "- **Sweep harnesses**: Code to run parameter sweeps over D, reps, trainable vs fixed QRF\n",
    "\n",
    "These are **NOT required** for the main paper results. All canonical results come from:\n",
    "- QRFPre (fixed quantum random features)\n",
    "- QKernelLandmarksPre (quantum kernel landmarks)\n",
    "- blend_cv with these preprocessors\n",
    "\n",
    "Run **Part 0-8 above** to get the main quantum augmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfe4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental: TrainableQRFPre (kept for reproducibility)\n",
    "<VSCode.Cell id=\"#VSC-5cfe73f9\" language=\"python\">\n",
    "# EXPERIMENTAL: small QRF sweep harness (keeps results local)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def run_qrf_sweep(blend_cv_fn, X, y_reg, groups, FIXED_SPLITS, LIGHT_FEATS, eat_exceptions=True):\n",
    "    settings = [(6,1),(8,2),(12,2)]\n",
    "    results = {}\n",
    "    for trainable in (False, True):\n",
    "        for D,reps in settings:\n",
    "            try:\n",
    "                if trainable:\n",
    "                    pre = Pipeline([( 'winsor', WinsorClipper(1,99)), ('tqrf', TrainableQRFPre(D=D,reps=reps,seed=11,epochs=30,lr=0.05,concat=True))])\n",
    "                else:\n",
    "                    pre = Pipeline([( 'winsor', WinsorClipper(1,99)), ('qrf', QRFPre(D=D,reps=reps,seed=11,concat=True))])\n",
    "                print(f\"\\n== QRF sweep: D={D} reps={reps} trainable={trainable} ==\")\n",
    "                bag = blend_cv_fn(X, y_reg, groups, FIXED_SPLITS, pre=pre, lgbm_seeds=(0,1,2), use_cat=True, use_xgb=True, feat_df=None, light_feat_cols=[])\n",
    "                results[(D,reps,trainable)] = bag\n",
    "            except Exception as e:\n",
    "                print('Sweep step failed:', e)\n",
    "                if not eat_exceptions:\n",
    "                    raise\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b572c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
